{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwJrLYhn8FASzATVM8PRSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaHolmlund/M6_Group_Assignments/blob/main/Group_Assignment_1/Group_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "\n",
        "Develop a Proof-of-Concept version of an application that is querying a database to come provide an output to the user."
      ],
      "metadata": {
        "id": "uUxiGoVQIwKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be for example:\n",
        "- Selecting observations from database, performing prediction with a (beforehand fitted) SML model.\n",
        "- Perform a UML procedure on observations queried from a database.\n",
        "- Perform a semantic/similarity search for an user input, retrieve most similar docs from a database.\n",
        "\n",
        "The data used should be non-trivial (eg.: enough observations,´maybe multiple tables, different types of data…)\n",
        " - The solution has to be self-contained. This can be done:\n",
        " - Within a colab using for grad.io. (Hint: An option is to save the database on github, and then load it in the colab).)\n",
        " - As a streamlit app (figure out how to make it self-contained).\n",
        " - (sky is the limit.)\n",
        "\n",
        "Possible databases:\n",
        "- SQL DB (eg. SQL-lite)\n",
        "- NoSQL DB\n",
        " - Document (eg. tinyDB)\n",
        " - Vector (Eg. Faiss, Chroma)"
      ],
      "metadata": {
        "id": "MixgZ6VgIzCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution"
      ],
      "metadata": {
        "id": "b2-BSVkKI4Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we have created a SQLite database containing information about the 2.000 most cited documents on Scopus within the topic of Natural Language Processing.\n",
        "\n",
        "Subsequently, a T5 summarization pipeline has been applied from HuggingFace to generate super-summarized abstracts of 50 characters or less. This provides users with a quick overview of the documents' content. The model is demonstrated in Grad.io in which the user can select a document from the Scopus database or upload their own abstract/abstracts not available in the database. The interface then returns a super-summarized abstract."
      ],
      "metadata": {
        "id": "W1SKVFwtI55F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "UvBbIiEkJcDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --q\n",
        "!pip install transformers --q"
      ],
      "metadata": {
        "id": "d8wO5bA8dslr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "from IPython.display import Image\n",
        "\n",
        "pd.set_option('max_colwidth', 1000)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T15GHfLTJbIQ",
        "outputId": "6d59a821-144f-4738-bf7f-697e5b827162"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the database"
      ],
      "metadata": {
        "id": "Go0rY-GbJKYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h9niVPdXIQtb"
      },
      "outputs": [],
      "source": [
        "# Reading the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/NadiaHolmlund/M6_Group_Assignments/main/Group_Assignment_1/Data/Scopus_NLP.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNpzpIc-NUPe",
        "outputId": "5ae6e838-d59c-4ad0-8ba2-7ae3f24ec7d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                    Authors  \\\n",
              "0                                                    Pennington J., Socher R., Manning C.D.   \n",
              "1                                              Devlin J., Chang M.-W., Lee K., Toutanova K.   \n",
              "2  Cho K., Van Merriënboer B., Gulcehre C., Bahdanau D., Bougares F., Schwenk H., Bengio Y.   \n",
              "3                                                         Pang B., Lee L., Vaithyanathan S.   \n",
              "4                   Collobert R., Weston J., Bottou L., Karlen M., Kavukcuoglu K., Kuksa P.   \n",
              "\n",
              "                                                                         Author(s) ID  \\\n",
              "0                                                22953926600;24766896100;35280197500;   \n",
              "1                                     54879967400;25925685700;56349980800;6506107920;   \n",
              "2  55722769200;57188495900;56006846900;57188434700;42061073000;7005072756;7003958245;   \n",
              "3                                                   8644537200;7404389769;6603253116;   \n",
              "4              14064641400;8865128200;6701721644;25651854400;25646533000;57221708009;   \n",
              "\n",
              "                                                                                           Title  \\\n",
              "0                                                  GloVe: Global vectors for word representation   \n",
              "1               BERT: Pre-training of deep bidirectional transformers for language understanding   \n",
              "2  Learning phrase representations using RNN encoder-decoder for statistical machine translation   \n",
              "3                          Thumbs up? Sentiment Classification using Machine Learning Techniques   \n",
              "4                                              Natural language processing (almost) from scratch   \n",
              "\n",
              "   Year  \\\n",
              "0  2014   \n",
              "1  2019   \n",
              "2  2014   \n",
              "3  2002   \n",
              "4  2011   \n",
              "\n",
              "                                                                                                                                                                   Source title  \\\n",
              "0                                                               EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference   \n",
              "1  NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference   \n",
              "2                                                               EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference   \n",
              "3                                                                            Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002   \n",
              "4                                                                                                                                          Journal of Machine Learning Research   \n",
              "\n",
              "  Volume Issue Art. No. Page start Page end  ...           ISBN  CODEN  \\\n",
              "0    NaN   NaN      NaN       1532     1543  ...  9781937284961    NaN   \n",
              "1      1   NaN      NaN       4171     4186  ...  9781950737130    NaN   \n",
              "2    NaN   NaN      NaN       1724     1734  ...  9781937284961    NaN   \n",
              "3    NaN   NaN      NaN         79       86  ...            NaN    NaN   \n",
              "4     12   NaN      NaN       2493     2537  ...            NaN    NaN   \n",
              "\n",
              "  PubMed ID Language of Original Document  \\\n",
              "0       NaN                       English   \n",
              "1       NaN                       English   \n",
              "2       NaN                       English   \n",
              "3       NaN                       English   \n",
              "4       NaN                       English   \n",
              "\n",
              "                                                                         Abbreviated Source Title  \\\n",
              "0                                   EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc. Conf.   \n",
              "1  NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.   \n",
              "2                                   EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc. Conf.   \n",
              "3                                           Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP   \n",
              "4                                                                            J. Mach. Learn. Res.   \n",
              "\n",
              "      Document Type Publication Stage             Open Access  Source  \\\n",
              "0  Conference Paper             Final                     NaN  Scopus   \n",
              "1  Conference Paper             Final                     NaN  Scopus   \n",
              "2  Conference Paper             Final  All Open Access, Green  Scopus   \n",
              "3  Conference Paper             Final                     NaN  Scopus   \n",
              "4           Article             Final                     NaN  Scopus   \n",
              "\n",
              "                  EID  \n",
              "0  2-s2.0-84961289992  \n",
              "1  2-s2.0-85083815650  \n",
              "2  2-s2.0-84961291190  \n",
              "3  2-s2.0-85141803251  \n",
              "4  2-s2.0-80053558787  \n",
              "\n",
              "[5 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb620195-d494-451a-8a6a-0016cddcff35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Authors</th>\n",
              "      <th>Author(s) ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Year</th>\n",
              "      <th>Source title</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Art. No.</th>\n",
              "      <th>Page start</th>\n",
              "      <th>Page end</th>\n",
              "      <th>...</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>CODEN</th>\n",
              "      <th>PubMed ID</th>\n",
              "      <th>Language of Original Document</th>\n",
              "      <th>Abbreviated Source Title</th>\n",
              "      <th>Document Type</th>\n",
              "      <th>Publication Stage</th>\n",
              "      <th>Open Access</th>\n",
              "      <th>Source</th>\n",
              "      <th>EID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pennington J., Socher R., Manning C.D.</td>\n",
              "      <td>22953926600;24766896100;35280197500;</td>\n",
              "      <td>GloVe: Global vectors for word representation</td>\n",
              "      <td>2014</td>\n",
              "      <td>EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1532</td>\n",
              "      <td>1543</td>\n",
              "      <td>...</td>\n",
              "      <td>9781937284961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>English</td>\n",
              "      <td>EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc. Conf.</td>\n",
              "      <td>Conference Paper</td>\n",
              "      <td>Final</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Scopus</td>\n",
              "      <td>2-s2.0-84961289992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Devlin J., Chang M.-W., Lee K., Toutanova K.</td>\n",
              "      <td>54879967400;25925685700;56349980800;6506107920;</td>\n",
              "      <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>\n",
              "      <td>2019</td>\n",
              "      <td>NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4171</td>\n",
              "      <td>4186</td>\n",
              "      <td>...</td>\n",
              "      <td>9781950737130</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>English</td>\n",
              "      <td>NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.</td>\n",
              "      <td>Conference Paper</td>\n",
              "      <td>Final</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Scopus</td>\n",
              "      <td>2-s2.0-85083815650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cho K., Van Merriënboer B., Gulcehre C., Bahdanau D., Bougares F., Schwenk H., Bengio Y.</td>\n",
              "      <td>55722769200;57188495900;56006846900;57188434700;42061073000;7005072756;7003958245;</td>\n",
              "      <td>Learning phrase representations using RNN encoder-decoder for statistical machine translation</td>\n",
              "      <td>2014</td>\n",
              "      <td>EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1724</td>\n",
              "      <td>1734</td>\n",
              "      <td>...</td>\n",
              "      <td>9781937284961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>English</td>\n",
              "      <td>EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc. Conf.</td>\n",
              "      <td>Conference Paper</td>\n",
              "      <td>Final</td>\n",
              "      <td>All Open Access, Green</td>\n",
              "      <td>Scopus</td>\n",
              "      <td>2-s2.0-84961291190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pang B., Lee L., Vaithyanathan S.</td>\n",
              "      <td>8644537200;7404389769;6603253116;</td>\n",
              "      <td>Thumbs up? Sentiment Classification using Machine Learning Techniques</td>\n",
              "      <td>2002</td>\n",
              "      <td>Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>79</td>\n",
              "      <td>86</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>English</td>\n",
              "      <td>Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</td>\n",
              "      <td>Conference Paper</td>\n",
              "      <td>Final</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Scopus</td>\n",
              "      <td>2-s2.0-85141803251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Collobert R., Weston J., Bottou L., Karlen M., Kavukcuoglu K., Kuksa P.</td>\n",
              "      <td>14064641400;8865128200;6701721644;25651854400;25646533000;57221708009;</td>\n",
              "      <td>Natural language processing (almost) from scratch</td>\n",
              "      <td>2011</td>\n",
              "      <td>Journal of Machine Learning Research</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2493</td>\n",
              "      <td>2537</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>English</td>\n",
              "      <td>J. Mach. Learn. Res.</td>\n",
              "      <td>Article</td>\n",
              "      <td>Final</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Scopus</td>\n",
              "      <td>2-s2.0-80053558787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb620195-d494-451a-8a6a-0016cddcff35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb620195-d494-451a-8a6a-0016cddcff35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb620195-d494-451a-8a6a-0016cddcff35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting columns to be included in the database\n",
        "df_clean = df[['Authors', 'Title', 'Abstract']]"
      ],
      "metadata": {
        "id": "YXAAqeIUx0xL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining the DataFrame\n",
        "df_clean.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM2KJcwQM7WP",
        "outputId": "321b03e4-a21b-4409-ff8d-a2d6070d8842"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Authors   2000 non-null   object\n",
            " 1   Title     2000 non-null   object\n",
            " 2   Abstract  2000 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting columns from objects to strings\n",
        "df_clean['Authors'] = df_clean['Authors'].astype(str)\n",
        "df_clean['Title'] = df_clean['Title'].astype(str)\n",
        "df_clean['Abstract'] = df_clean['Abstract'].astype(str)"
      ],
      "metadata": {
        "id": "HYMF9rK6Lo8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-examining the dataframe\n",
        "df_clean.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In1qGLC-1Qyr",
        "outputId": "ccab190a-1246-447c-bde6-12cc4e37ab93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Authors   2000 non-null   object\n",
            " 1   Title     2000 non-null   object\n",
            " 2   Abstract  2000 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a connection to the database\n",
        "conn = sqlite3.connect('Scopus.db')\n",
        "\n",
        "# Setting up a cursor (pointer to rows in database)\n",
        "c = conn.cursor()"
      ],
      "metadata": {
        "id": "BEqabAFJL0LK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new table for the 'Authors' column\n",
        "create_authors_query = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS authors_table (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        author TEXT\n",
        "    );\n",
        "\"\"\"\n",
        "conn.execute(create_authors_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK6QJAppL20y",
        "outputId": "4150369a-9d49-4c49-bd4e-f9c03ebc9c78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7ff31c8f8bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new table for the 'Title' column\n",
        "create_title_query = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS title_table (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        title TEXT,\n",
        "        author TEXT,\n",
        "        FOREIGN KEY (author) REFERENCES authors_table (id)\n",
        "    );\n",
        "\"\"\"\n",
        "conn.execute(create_title_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G138BnsGL4nk",
        "outputId": "4e9b1052-fcdd-4313-edb5-26e893497362"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7ff31c8f8d40>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new table for the 'Abstract' column\n",
        "create_abstract_query = \"\"\"\n",
        "    CREATE TABLE abstract_table (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    abstract TEXT,\n",
        "    title TEXT,\n",
        "    summary TEXT,\n",
        "    FOREIGN KEY (title) REFERENCES title_table(title)\n",
        ");\n",
        "\"\"\"\n",
        "conn.execute(create_abstract_query)"
      ],
      "metadata": {
        "id": "pIL2aSZwL-E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c365ef83-05c9-42b9-f5af-9724564e0fc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7ff31c8f8e40>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inserting data into the 'authors_table'\n",
        "authors = set()\n",
        "for author_str in df_clean['Authors']:\n",
        "    for author in author_str.split(','):\n",
        "        authors.add(author.strip())\n",
        "for author in authors:\n",
        "    insert_author_query = \"INSERT OR IGNORE INTO authors_table (author) VALUES (?);\"\n",
        "    conn.execute(insert_author_query, (author,))\n"
      ],
      "metadata": {
        "id": "vWwocOhT8V9Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inserting data into the 'title_table'\n",
        "select_authors_query = \"SELECT * FROM authors_table;\"\n",
        "authors = {row[1]: row[0] for row in conn.execute(select_authors_query)}\n",
        "for title, author_str in zip(df_clean['Title'], df_clean['Authors']):\n",
        "    title = title.replace(\"'\", \"''\")\n",
        "    insert_title_query = f\"INSERT OR IGNORE INTO title_table (title, author) VALUES ('{title}', '{authors[author_str.split(', ')[0]]}');\"\n",
        "    conn.execute(insert_title_query)"
      ],
      "metadata": {
        "id": "JSUlk02C8eyT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inserting data into the 'abstract_table'\n",
        "select_titles_query = \"SELECT * FROM title_table;\"\n",
        "titles = {row[1]: row[0] for row in conn.execute(select_titles_query)}\n",
        "for abstract, title in zip(df_clean['Abstract'], df_clean['Title']):\n",
        "    insert_abstract_query = \"INSERT INTO abstract_table (abstract, title, summary) VALUES (?, ?, '');\"\n",
        "    conn.execute(insert_abstract_query, (abstract, titles[title]))\n"
      ],
      "metadata": {
        "id": "HTV0ojJXFmun"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting data from the tables to determine if JOIN works across the tables as intended\n",
        "select_query = \"\"\"\n",
        "    SELECT authors_table.author, title_table.title, abstract_table.abstract\n",
        "    FROM authors_table\n",
        "    JOIN title_table ON authors_table.id = title_table.author\n",
        "    JOIN abstract_table ON title_table.id = abstract_table.title\n",
        "    LIMIT 5;\n",
        "\"\"\"\n",
        "cursor = conn.execute(select_query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "for row in rows:\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "K9uTR3JhAQ97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50085615-e56f-4105-8376-baf31c7fd893"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Pennington J.', 'GloVe: Global vectors for word representation', 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.')\n",
            "('Devlin J.', 'BERT: Pre-training of deep bidirectional transformers for language understanding', 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics')\n",
            "('Cho K.', 'Learning phrase representations using RNN encoder-decoder for statistical machine translation', 'In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. © 2014 Association for Computational Linguistics.')\n",
            "('Pang B.', 'Thumbs up? Sentiment Classification using Machine Learning Techniques', 'We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. © 2002 Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002. All rights reserved.')\n",
            "('Collobert R.', 'Natural language processing (almost) from scratch', 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. © 2011 Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing an overview of the tables\n",
        "# Quering the schema for each table from the `sqlite_master` table\n",
        "c.execute(\"SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
        "schemas = c.fetchall()\n",
        "\n",
        "# Printing the schema for each table\n",
        "for schema in schemas:\n",
        "    table_name, table_schema = schema\n",
        "    print(f\"Table: {table_name}\\nSchema:\\n{table_schema}\\n\")"
      ],
      "metadata": {
        "id": "M0P17tZ-MSPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930d5d43-46f1-44ab-a74b-1adf9e70a8e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table: abstract_table\n",
            "Schema:\n",
            "CREATE TABLE abstract_table (\n",
            "    id INTEGER PRIMARY KEY,\n",
            "    abstract TEXT,\n",
            "    title TEXT,\n",
            "    summary TEXT,\n",
            "    FOREIGN KEY (title) REFERENCES title_table(title)\n",
            ")\n",
            "\n",
            "Table: authors_table\n",
            "Schema:\n",
            "CREATE TABLE authors_table (\n",
            "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
            "        author TEXT\n",
            "    )\n",
            "\n",
            "Table: sqlite_sequence\n",
            "Schema:\n",
            "CREATE TABLE sqlite_sequence(name,seq)\n",
            "\n",
            "Table: title_table\n",
            "Schema:\n",
            "CREATE TABLE title_table (\n",
            "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
            "        title TEXT,\n",
            "        author TEXT,\n",
            "        FOREIGN KEY (author) REFERENCES authors_table (id)\n",
            "    )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing information from the authors_table to test the database\n",
        "c.execute(\"SELECT * FROM authors_table LIMIT 5\")\n",
        "authors_table = c.fetchall()\n",
        "for author_name in authors_table:\n",
        "    print(author_name)"
      ],
      "metadata": {
        "id": "5L4XghiwMUi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6e7416-3135-4cac-adef-f88d4ec38b64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'Lehar J.')\n",
            "(2, 'Wu A.')\n",
            "(3, 'George S.')\n",
            "(4, 'Zaragoza H.')\n",
            "(5, 'Herbert C.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing information from the title_table to test the database\n",
        "c.execute(\"SELECT * FROM title_table LIMIT 5\")\n",
        "title_table = c.fetchall()\n",
        "for title in title_table:\n",
        "    print(title)"
      ],
      "metadata": {
        "id": "CrWJ7p1WMXMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910777fa-10cf-48a9-d53a-05289c573ed0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'GloVe: Global vectors for word representation', '377')\n",
            "(2, 'BERT: Pre-training of deep bidirectional transformers for language understanding', '5130')\n",
            "(3, 'Learning phrase representations using RNN encoder-decoder for statistical machine translation', '42')\n",
            "(4, 'Thumbs up? Sentiment Classification using Machine Learning Techniques', '4380')\n",
            "(5, 'Natural language processing (almost) from scratch', '1200')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing information from the abstract_table to test the database\n",
        "c.execute(\"SELECT * FROM abstract_table LIMIT 5\")\n",
        "abstract_table = c.fetchall()\n",
        "for abstract in abstract_table:\n",
        "    print(abstract)"
      ],
      "metadata": {
        "id": "16p9rJVKMZb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad3019b-c884-42c6-ed3a-9748a0b66eed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.', '1', '')\n",
            "(2, 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics', '2', '')\n",
            "(3, 'In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. © 2014 Association for Computational Linguistics.', '3', '')\n",
            "(4, 'We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. © 2002 Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002. All rights reserved.', '4', '')\n",
            "(5, 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. © 2011 Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.', '5', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0falMMadvxTq"
      },
      "source": [
        "# Summarization Pipeline using SQLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7XjnDCrsuLV"
      },
      "outputs": [],
      "source": [
        "# Loading a pre-trained text-summarization model from HuggingFace Transformers\n",
        "summarizer = pipeline('summarization', model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4unW87qFx8EV"
      },
      "outputs": [],
      "source": [
        "# Extracting abstracts for summarization\n",
        "abstracts = conn.execute('SELECT abstract FROM abstract_table limit 10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fomc32lTs6Wh"
      },
      "outputs": [],
      "source": [
        "# Iterating over the abstracts and updating the summary for each one\n",
        "for i, row in enumerate(abstracts):\n",
        "    # Extracting the text of the current abstract\n",
        "    abstract = row[0]\n",
        "    \n",
        "    # Summarizing the abstract using the pre-trained summarizer\n",
        "    summary = summarizer(abstract, max_length=50, min_length=0, do_sample=False)[0]['summary_text']\n",
        "    \n",
        "    # Updating the 'summary' column in the abstract_table with the summary of the current abstract\n",
        "    conn.execute('UPDATE abstract_table SET summary = ? WHERE rowid = ?', (summary, i+1))\n",
        "    \n",
        "# Commit the changes to the database\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BRz3jMbYvO_I"
      },
      "outputs": [],
      "source": [
        "# Define the SQL query\n",
        "query = 'SELECT * FROM abstract_table LIMIT 10'\n",
        "\n",
        "# Execute the query and convert the result to a DataFrame\n",
        "df_summed = pd.read_sql_query(query, conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N6XefZQkTVRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a16ee3db-f9f6-4b53-e4b7-43ff9f0fe4b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Examining the first abstract\n",
        "df_summed['abstract'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VXi3QRNoTWx9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2ccc95c6-d321-40e4-d7a2-16def976f851"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a new global logbilinear regression model is developed . it combines the advantages of global matrix factorization and local context window methods . the model produces a vector space with meaningful substructure .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Examining the summarization of the first abstract\n",
        "df_summed['summary'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on above, we determine that the model summarizes the abstract to a satisfying extent."
      ],
      "metadata": {
        "id": "uadSJUolea58"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njQND0oKdqaF"
      },
      "source": [
        "# Grad.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3U0MZTfkGJe"
      },
      "source": [
        "## Generate a summary of a title available in our Scopus Database"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, there is an issue with the Grad.io interface, which causes the output to fail when submitting a Title from the dropdown menu. However, the code functions without the interface, hence we suspect the issue is related to the dropdown function in Grad.io.\n",
        "\n",
        "The code is therefore presented with examples below for proof-of-concept, followed by the Grad.io interface design."
      ],
      "metadata": {
        "id": "jIuXyX2NelSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained text-summarization model from HuggingFace Transformers\n",
        "summarizer = pipeline('summarization', model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "\n",
        "# Connecting to the SQLite database\n",
        "conn = sqlite3.connect('Scopus.db')\n",
        "\n",
        "# Defining the dropdown options\n",
        "c = conn.cursor()\n",
        "c.execute(\"SELECT DISTINCT title FROM title_table\")\n",
        "dropdown_options = [row[0] for row in c.fetchall()]\n",
        "\n",
        "# Defining the function for summarization\n",
        "def summary(selected_option):\n",
        "    c.execute(\"SELECT abstract FROM abstract_table WHERE id = (SELECT id FROM title_table WHERE title = ?)\", (selected_option,))\n",
        "    abstract = c.fetchone()[0]\n",
        "    summary = summarizer(abstract, max_length=50, min_length=0, do_sample=False)\n",
        "    return summary[0]['summary_text']"
      ],
      "metadata": {
        "id": "0ZxxEEghfWsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first three titles for proof-of-concept purposes\n",
        "print(dropdown_options[0])\n",
        "print(dropdown_options[1])\n",
        "print(dropdown_options[2])"
      ],
      "metadata": {
        "id": "XIXGezEvfldR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ec65f5-58a8-427a-bfdc-cd308f2dc538"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe: Global vectors for word representation\n",
            "BERT: Pre-training of deep bidirectional transformers for language understanding\n",
            "Learning phrase representations using RNN encoder-decoder for statistical machine translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first three abstracts for proof-of-concept purposes\n",
        "c.execute(\"SELECT * FROM abstract_table LIMIT 3\")\n",
        "abstract_table = c.fetchall()\n",
        "for abstract in abstract_table:\n",
        "    print(abstract)"
      ],
      "metadata": {
        "id": "AgLX8_x3fsn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097af5c9-51a7-438e-f926-cd98a26d8ab5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.', '1', 'a new global logbilinear regression model is developed . it combines the advantages of global matrix factorization and local context window methods . the model produces a vector space with meaningful substructure .')\n",
            "(2, 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics', '2', 'a new language representation model is called BERT, which stands for Bidirectional Encoder Representations from Transformers . the pre-trained model can be fine-tuned with just one additional output layer .')\n",
            "(3, 'In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. © 2014 Association for Computational Linguistics.', '3', 'the proposed model consists of two recurrent neural networks . one encodes a sequence of symbols into a fixedlength vector representation . the other decodes the representation into another sequence .')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the summarization of the first three abstract for proof-of-concept purposes\n",
        "print(summary(dropdown_options[0]))\n",
        "print(summary(dropdown_options[1]))\n",
        "print(summary(dropdown_options[2]))"
      ],
      "metadata": {
        "id": "bW_87Zo6fldS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8391a422-d53f-46c1-d589-c06f74aba738"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:745: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a new global logbilinear regression model is developed . it combines the advantages of global matrix factorization and local context window methods . the model produces a vector space with meaningful substructure .\n",
            "a new language representation model is called BERT, which stands for Bidirectional Encoder Representations from Transformers . the pre-trained model can be fine-tuned with just one additional output layer .\n",
            "the proposed model consists of two recurrent neural networks . one encodes a sequence of symbols into a fixedlength vector representation . the other decodes the representation into another sequence .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfortunately, this part of the code fails when submitting a title from the dropdown menu. We suspect the issue is related to the Grad.io dropdown function.\n",
        "\n",
        "# Creating the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    title=\"Generate a summary of a title available in our Scopus Database\",\n",
        "    fn=summary,\n",
        "    inputs=gr.inputs.Dropdown(choices=dropdown_options, type=\"value\", label=\"Select a Title from Scopus\"),\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Summary\"),\n",
        ")\n",
        "\n",
        "# Launching the interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "AHYq6Yew9RXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is the full code for the Grad.io application\n",
        "\n",
        "# Loading a pre-trained text-summarization model from HuggingFace Transformers\n",
        "summarizer = pipeline('summarization', model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "\n",
        "# Connecting to the SQLite database\n",
        "conn = sqlite3.connect('Scopus.db')\n",
        "\n",
        "# Defining the dropdown options\n",
        "c = conn.cursor()\n",
        "c.execute(\"SELECT DISTINCT title FROM title_table\")\n",
        "dropdown_options = [row[0] for row in c.fetchall()]\n",
        "\n",
        "# Defining the function for summarization\n",
        "def summary(selected_option):\n",
        "    c.execute(\"SELECT abstract FROM abstract_table WHERE id = (SELECT id FROM title_table WHERE title = ?)\", (selected_option,))\n",
        "    abstract = c.fetchone()[0]\n",
        "    summary = summarizer(abstract, max_length=50, min_length=0, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Creating the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    title=\"Generate a summary of a title available in our Scopus Database\",\n",
        "    fn=summary,\n",
        "    inputs=gr.inputs.Dropdown(choices=dropdown_options, type=\"value\", label=\"Select a Title from Scopus\"),\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Summary\"),\n",
        ")\n",
        "\n",
        "# Launching the interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "0BlwCBSajuS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tpT500aj7-b"
      },
      "source": [
        "## Generate a summary of your own abstract/abstracts not available in our Scopus Database"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this interface is not connected to the Scopus database, since the input is user-defined."
      ],
      "metadata": {
        "id": "sfVb-N-VhCmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrm4fFSdgVbE"
      },
      "outputs": [],
      "source": [
        "# Loading a pre-trained text-summarization model from HuggingFace Transformers\n",
        "summarizer = pipeline('summarization', model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "\n",
        "# Defining the function for summarization\n",
        "def summary(abstract):\n",
        "    summary = summarizer(abstract, max_length=50, min_length=0, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Defining examples\n",
        "examples = [\n",
        "    [\"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.\"],\n",
        "    [\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics\"],\n",
        "    [\"In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. © 2014 Association for Computational Linguistics.\"],\n",
        "]\n",
        "\n",
        "# Creating the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    title=\"Generate a summary of your own abstract/abstracts not available in our Scopus Database\",\n",
        "    fn=summary,\n",
        "    inputs=gr.inputs.Textbox(lines=5, label=\"Input Abstract\"),\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Summary\"),\n",
        "    examples=examples\n",
        ")\n",
        "\n",
        "# Launching the interface\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing the Database Connection"
      ],
      "metadata": {
        "id": "RHnLwin5yvSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn.close()"
      ],
      "metadata": {
        "id": "x_Bs4fzUyy4e"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}